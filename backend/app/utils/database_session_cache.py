"""
Database Session Caching System for User Data (Phase 4)
========================================================

This module implements a comprehensive user session caching system with 15-minute TTL
to reduce database load by 80-90% while maintaining data consistency. It provides
smart caching strategies specifically designed for user data synchronization with Clerk.

Key Features:
1. User session caching with configurable TTL (default 15 minutes)
2. Smart change detection based on Clerk's updated_at timestamps
3. Cache invalidation strategies for user data updates
4. Database connection pooling optimization
5. Performance monitoring integration
6. Thread-safe cache operations with proper error handling

Architecture:
- Level 1: In-memory user session cache (15min TTL)
- Level 2: Database connection pooling optimization
- Level 3: Smart sync detection to minimize database writes
- Level 4: Cache invalidation and consistency management
"""

import asyncio
import logging
import time
import threading
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List, Union, Callable
from dataclasses import dataclass, field
from collections import defaultdict
import hashlib
import json
from contextlib import asynccontextmanager

import httpx
from sqlalchemy import text, event
from sqlalchemy.orm import Session
from sqlalchemy.pool import QueuePool
from fastapi import HTTPException, status

from ..models.user import User
from ..utils.database import get_db, engine
from ..core.config import settings
from .auth_cache import TTLCache, CacheMetrics

logger = logging.getLogger(__name__)

# ============================================================================
# USER SESSION DATA STRUCTURES
# ============================================================================

@dataclass
class UserSessionData:
    """User session data structure with metadata for caching"""
    user_id: int
    clerk_user_id: str
    email: str
    first_name: Optional[str] = None
    last_name: Optional[str] = None
    is_active: bool = True
    onboarding_completed: bool = False
    last_clerk_sync: Optional[datetime] = None
    clerk_updated_at: Optional[str] = None
    cached_at: datetime = field(default_factory=datetime.now)
    db_synced: bool = True
    profile_data: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return {
            'user_id': self.user_id,
            'clerk_user_id': self.clerk_user_id,
            'email': self.email,
            'first_name': self.first_name,
            'last_name': self.last_name,
            'is_active': self.is_active,
            'onboarding_completed': self.onboarding_completed,
            'last_clerk_sync': self.last_clerk_sync.isoformat() if self.last_clerk_sync else None,
            'clerk_updated_at': self.clerk_updated_at,
            'cached_at': self.cached_at.isoformat(),
            'db_synced': self.db_synced,
            'profile_data': self.profile_data or {}
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'UserSessionData':
        """Create from dictionary"""
        return cls(
            user_id=data['user_id'],
            clerk_user_id=data['clerk_user_id'],
            email=data['email'],
            first_name=data.get('first_name'),
            last_name=data.get('last_name'),
            is_active=data.get('is_active', True),
            onboarding_completed=data.get('onboarding_completed', False),
            last_clerk_sync=datetime.fromisoformat(data['last_clerk_sync']) if data.get('last_clerk_sync') else None,
            clerk_updated_at=data.get('clerk_updated_at'),
            cached_at=datetime.fromisoformat(data['cached_at']),
            db_synced=data.get('db_synced', True),
            profile_data=data.get('profile_data')
        )
    
    def needs_sync(self, clerk_data: Dict[str, Any]) -> bool:
        """Determine if user data needs sync with database based on Clerk timestamps"""
        if not self.clerk_updated_at:
            return True  # No previous sync timestamp
        
        clerk_updated_at = clerk_data.get('updated_at')
        if not clerk_updated_at:
            return False  # No update timestamp from Clerk
        
        # Compare timestamps (Clerk uses ISO format)
        try:
            clerk_time = datetime.fromisoformat(clerk_updated_at.replace('Z', '+00:00'))
            cached_time = datetime.fromisoformat(self.clerk_updated_at.replace('Z', '+00:00')) if self.clerk_updated_at else datetime.min
            return clerk_time > cached_time
        except (ValueError, AttributeError):
            logger.warning(f"Could not parse timestamps for sync comparison")
            return True  # Sync if unsure

# ============================================================================
# USER SESSION CACHE MANAGER
# ============================================================================

class UserSessionCache:
    """
    High-performance user session cache with intelligent sync detection.
    Reduces database load by caching user data with proper invalidation strategies.
    """
    
    def __init__(self, 
                 default_ttl: int = 900,  # 15 minutes
                 max_cache_size: int = 10000,
                 cleanup_interval: int = 300):  # 5 minutes cleanup
        """
        Initialize user session cache
        
        Args:
            default_ttl: Default cache TTL in seconds (900 = 15 minutes)
            max_cache_size: Maximum number of cached user sessions
            cleanup_interval: Interval between cache cleanup operations
        """
        self.default_ttl = default_ttl
        self.max_cache_size = max_cache_size
        self.cleanup_interval = cleanup_interval
        
        # Thread-safe cache storage
        self._cache: Dict[str, Dict[str, Any]] = {}
        self._lock = threading.RLock()
        
        # Performance monitoring
        self._stats = {
            'cache_hits': 0,
            'cache_misses': 0,
            'database_syncs': 0,
            'sync_skips': 0,
            'evictions': 0,
            'invalidations': 0
        }
        
        # Background cleanup task
        self._cleanup_task: Optional[asyncio.Task] = None
        self._cleanup_running = False
        
        logger.info(f"üîÑ UserSessionCache initialized (TTL: {default_ttl}s, Max size: {max_cache_size})")
    
    async def start_background_cleanup(self):
        """Start background cleanup task"""
        if self._cleanup_running:
            return
            
        self._cleanup_running = True
        self._cleanup_task = asyncio.create_task(self._cleanup_loop())
        logger.info("üßπ User session cache cleanup started")
    
    async def stop_background_cleanup(self):
        """Stop background cleanup task"""
        self._cleanup_running = False
        if self._cleanup_task:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
        logger.info("‚èπÔ∏è User session cache cleanup stopped")
    
    async def _cleanup_loop(self):
        """Background cleanup loop"""
        while self._cleanup_running:
            try:
                await asyncio.sleep(self.cleanup_interval)
                if not self._cleanup_running:
                    break
                
                self._cleanup_expired()
                self._enforce_size_limit()
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in cache cleanup loop: {e}")
                await asyncio.sleep(60)  # Wait before retrying
    
    def _cleanup_expired(self) -> int:
        """Remove expired cache entries"""
        current_time = time.time()
        expired_keys = []
        
        with self._lock:
            for key, entry in self._cache.items():
                if current_time > entry.get('expires_at', 0):
                    expired_keys.append(key)
            
            for key in expired_keys:
                del self._cache[key]
                self._stats['evictions'] += 1
        
        if expired_keys:
            logger.debug(f"üóëÔ∏è Cleaned up {len(expired_keys)} expired user sessions")
        
        return len(expired_keys)
    
    def _enforce_size_limit(self):
        """Enforce maximum cache size by removing oldest entries"""
        with self._lock:
            if len(self._cache) <= self.max_cache_size:
                return
            
            # Sort by access time, remove oldest
            entries = [(key, entry.get('last_accessed', 0)) for key, entry in self._cache.items()]
            entries.sort(key=lambda x: x[1])  # Sort by access time
            
            to_remove = len(entries) - self.max_cache_size
            for key, _ in entries[:to_remove]:
                del self._cache[key]
                self._stats['evictions'] += 1
            
            logger.debug(f"üóÇÔ∏è Evicted {to_remove} entries to maintain cache size limit")
    
    def get_user_session(self, clerk_user_id: str) -> Optional[UserSessionData]:
        """Get user session from cache"""
        cache_key = f"user_session:{clerk_user_id}"
        
        with self._lock:
            if cache_key not in self._cache:
                self._stats['cache_misses'] += 1
                logger.debug(f"üîç User session cache miss: {clerk_user_id}")
                return None
            
            entry = self._cache[cache_key]
            
            # Check if expired
            if time.time() > entry.get('expires_at', 0):
                del self._cache[cache_key]
                self._stats['evictions'] += 1
                self._stats['cache_misses'] += 1
                logger.debug(f"‚è∞ User session cache expired: {clerk_user_id}")
                return None
            
            # Update access time
            entry['last_accessed'] = time.time()
            self._stats['cache_hits'] += 1
            
            # Convert back to UserSessionData
            session_data = UserSessionData.from_dict(entry['data'])
            logger.debug(f"üéØ User session cache hit: {clerk_user_id}")
            return session_data
    
    def set_user_session(self, 
                        clerk_user_id: str, 
                        session_data: UserSessionData, 
                        ttl: Optional[int] = None) -> None:
        """Set user session in cache"""
        cache_key = f"user_session:{clerk_user_id}"
        ttl = ttl or self.default_ttl
        expires_at = time.time() + ttl
        
        with self._lock:
            self._cache[cache_key] = {
                'data': session_data.to_dict(),
                'expires_at': expires_at,
                'created_at': time.time(),
                'last_accessed': time.time()
            }
        
        logger.debug(f"üíæ User session cached: {clerk_user_id} (TTL: {ttl}s)")
    
    def invalidate_user_session(self, clerk_user_id: str) -> bool:
        """Invalidate specific user session"""
        cache_key = f"user_session:{clerk_user_id}"
        
        with self._lock:
            if cache_key in self._cache:
                del self._cache[cache_key]
                self._stats['invalidations'] += 1
                logger.debug(f"üóëÔ∏è User session invalidated: {clerk_user_id}")
                return True
            return False
    
    def clear_all_sessions(self) -> int:
        """Clear all cached sessions"""
        with self._lock:
            count = len(self._cache)
            self._cache.clear()
            logger.warning(f"üßπ All user sessions cleared: {count} entries")
            return count
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        with self._lock:
            hit_rate = (
                self._stats['cache_hits'] / (self._stats['cache_hits'] + self._stats['cache_misses'])
                if (self._stats['cache_hits'] + self._stats['cache_misses']) > 0 else 0
            )
            
            return {
                **self._stats,
                'cache_size': len(self._cache),
                'hit_rate': hit_rate,
                'max_cache_size': self.max_cache_size,
                'default_ttl': self.default_ttl
            }

# ============================================================================
# SMART DATABASE SYNC MANAGER
# ============================================================================

class SmartDatabaseSync:
    """
    Smart database synchronization manager that minimizes database writes
    by detecting actual changes in user data using Clerk timestamps.
    """
    
    def __init__(self, session_cache: UserSessionCache):
        self.session_cache = session_cache
        self._sync_stats = {
            'sync_attempts': 0,
            'sync_performed': 0,
            'sync_skipped': 0,
            'sync_errors': 0,
            'profile_creates': 0
        }
        
    async def sync_user_with_database(self, 
                                    clerk_user_data: Dict[str, Any], 
                                    db: Session,
                                    force_sync: bool = False) -> UserSessionData:
        """
        Smart sync user data with database, only performing writes when necessary.
        
        Args:
            clerk_user_data: User data from Clerk API
            db: Database session
            force_sync: Force database sync regardless of timestamps
            
        Returns:
            UserSessionData with updated information
        """
        self._sync_stats['sync_attempts'] += 1
        clerk_user_id = clerk_user_data.get('id')
        
        if not clerk_user_id:
            raise ValueError("No Clerk user ID in data")
        
        # Check cache first
        cached_session = self.session_cache.get_user_session(clerk_user_id)
        
        # Determine if sync is needed
        needs_sync = (
            force_sync or
            cached_session is None or
            not cached_session.db_synced or
            cached_session.needs_sync(clerk_user_data)
        )
        
        if not needs_sync and cached_session:
            self._sync_stats['sync_skipped'] += 1
            logger.debug(f"‚ö° Skipping database sync for {clerk_user_id} (no changes detected)")
            return cached_session
        
        try:
            # Perform database synchronization
            user_data = await self._perform_database_sync(clerk_user_data, db)
            self._sync_stats['sync_performed'] += 1
            
            # Create session data
            session_data = UserSessionData(
                user_id=user_data['id'],
                clerk_user_id=clerk_user_id,
                email=user_data['email'],
                first_name=user_data.get('first_name'),
                last_name=user_data.get('last_name'),
                is_active=user_data.get('is_active', True),
                onboarding_completed=user_data.get('onboarding_completed', False),
                last_clerk_sync=datetime.now(),
                clerk_updated_at=clerk_user_data.get('updated_at'),
                db_synced=True,
                profile_data=user_data.get('profile_data')
            )
            
            # Cache the session
            self.session_cache.set_user_session(clerk_user_id, session_data)
            
            logger.info(f"‚úÖ User database sync completed: {clerk_user_id}")
            return session_data
            
        except Exception as e:
            self._sync_stats['sync_errors'] += 1
            logger.error(f"‚ùå Database sync failed for {clerk_user_id}: {e}")
            
            # Return cached data if available, otherwise raise
            if cached_session:
                logger.warning(f"‚ö†Ô∏è Returning cached data due to sync failure")
                return cached_session
            raise
    
    async def _perform_database_sync(self, 
                                   clerk_user_data: Dict[str, Any], 
                                   db: Session) -> Dict[str, Any]:
        """Perform the actual database synchronization"""
        clerk_user_id = clerk_user_data.get("id")
        
        # Extract email from Clerk data
        email = None
        if "email_addresses" in clerk_user_data:
            email_list = clerk_user_data.get("email_addresses", [])
            if email_list and len(email_list) > 0:
                email = email_list[0].get("email_address")
        elif "email" in clerk_user_data:
            email = clerk_user_data.get("email")
        
        if not email:
            raise ValueError(f"No email found for Clerk user {clerk_user_id}")
        
        # Check if user exists
        existing_user = db.query(User).filter(User.clerk_user_id == clerk_user_id).first()
        
        if not existing_user:
            # Try to find by email (migration case)
            existing_user = db.query(User).filter(User.email == email).first()
        
        if existing_user:
            # Update existing user
            updated_fields = []
            
            if existing_user.clerk_user_id != clerk_user_id:
                existing_user.clerk_user_id = clerk_user_id
                updated_fields.append('clerk_user_id')
            
            new_first_name = clerk_user_data.get("first_name")
            if existing_user.first_name != new_first_name:
                existing_user.first_name = new_first_name
                updated_fields.append('first_name')
            
            new_last_name = clerk_user_data.get("last_name")
            if existing_user.last_name != new_last_name:
                existing_user.last_name = new_last_name
                updated_fields.append('last_name')
            
            # Add last_clerk_sync timestamp
            existing_user.last_clerk_sync = datetime.now()
            
            if updated_fields:
                logger.debug(f"üìù Updating user fields: {updated_fields}")
                db.commit()
                db.refresh(existing_user)
            else:
                logger.debug(f"üìã No user updates needed for {clerk_user_id}")
            
            user_data = {
                "id": existing_user.id,
                "email": existing_user.email,
                "clerk_user_id": existing_user.clerk_user_id,
                "first_name": existing_user.first_name,
                "last_name": existing_user.last_name,
                "is_active": existing_user.is_active,
                "onboarding_completed": existing_user.onboarding_completed
            }
        else:
            # Create new user
            logger.info(f"üë§ Creating new user from Clerk: {email}")
            new_user = User(
                clerk_user_id=clerk_user_id,
                email=email,
                first_name=clerk_user_data.get("first_name"),
                last_name=clerk_user_data.get("last_name"),
                last_clerk_sync=datetime.now()
            )
            
            db.add(new_user)
            db.commit()
            db.refresh(new_user)
            
            user_data = {
                "id": new_user.id,
                "email": new_user.email,
                "clerk_user_id": new_user.clerk_user_id,
                "first_name": new_user.first_name,
                "last_name": new_user.last_name,
                "is_active": new_user.is_active,
                "onboarding_completed": new_user.onboarding_completed
            }
        
        # Ensure user profile exists
        await self._ensure_user_profile_exists(user_data['id'], db)
        
        return user_data
    
    async def _ensure_user_profile_exists(self, user_id: int, db: Session) -> None:
        """Ensure user profile exists, create if needed"""
        try:
            from ..models.user_profile import UserProfile
            
            existing_profile = db.query(UserProfile).filter(UserProfile.user_id == user_id).first()
            
            if not existing_profile:
                user = db.query(User).filter(User.id == user_id).first()
                if not user:
                    return
                
                logger.info(f"üë§ Creating user profile for user ID {user_id}")
                new_profile = UserProfile(
                    user_id=user_id,
                    name=f"{user.first_name} {user.last_name}".strip() if user.first_name or user.last_name else None,
                    # Set safe defaults
                    age=None, sex=None, major=None, year=None, gpa=None,
                    hobbies=None, country=None, state_province=None,
                    unique_quality=None, story=None, favorite_movie=None,
                    favorite_book=None, favorite_celebrities=None,
                    learning_style=None, interests=None, job_title=None,
                    industry=None, years_experience=None, education_level=None,
                    career_goals=None, skills=[], personal_analysis=None
                )
                
                db.add(new_profile)
                db.commit()
                db.refresh(new_profile)
                self._sync_stats['profile_creates'] += 1
                logger.info(f"‚úÖ User profile created for user ID {user_id}")
                
        except Exception as e:
            logger.error(f"Failed to create user profile for user ID {user_id}: {e}")
            db.rollback()
            raise
    
    def get_sync_stats(self) -> Dict[str, Any]:
        """Get synchronization statistics"""
        sync_rate = (
            self._sync_stats['sync_performed'] / self._sync_stats['sync_attempts']
            if self._sync_stats['sync_attempts'] > 0 else 0
        )
        skip_rate = (
            self._sync_stats['sync_skipped'] / self._sync_stats['sync_attempts']
            if self._sync_stats['sync_attempts'] > 0 else 0
        )
        
        return {
            **self._sync_stats,
            'sync_rate': sync_rate,
            'skip_rate': skip_rate
        }

# ============================================================================
# DATABASE CONNECTION POOL OPTIMIZER
# ============================================================================

class DatabaseConnectionOptimizer:
    """
    Optimizes database connection pooling for improved performance with user session caching.
    Provides monitoring and tuning of connection pool parameters.
    """
    
    def __init__(self):
        self._pool_stats = {
            'pool_size': 0,
            'checked_in': 0,
            'checked_out': 0,
            'overflow': 0,
            'total_connections': 0
        }
        self._monitoring_enabled = False
    
    def configure_optimized_pool(self) -> Dict[str, Any]:
        """Configure optimized connection pool settings"""
        # Base configuration optimized for user session caching
        pool_config = {
            "pool_size": 5,  # Moderate pool size for Railway/production
            "max_overflow": 10,  # Allow burst capacity
            "pool_timeout": 30,  # Connection timeout
            "pool_recycle": 3600,  # Recycle connections every hour
            "pool_pre_ping": True,  # Test connections before use
            "pool_reset_on_return": "commit"  # Reset on return
        }
        
        # Environment-specific optimizations
        if settings.is_railway:
            pool_config.update({
                "pool_size": 3,  # Smaller for Railway limits
                "max_overflow": 7,
                "pool_timeout": 20
            })
        
        if settings.is_production:
            pool_config.update({
                "pool_recycle": 7200,  # 2 hour recycle in production
                "echo": False  # Disable SQL logging
            })
        else:
            pool_config.update({
                "echo": False  # Enable for debugging if needed
            })
        
        logger.info(f"üîß Database pool optimized: {pool_config}")
        return pool_config
    
    def enable_pool_monitoring(self):
        """Enable connection pool monitoring"""
        if not engine:
            logger.warning("Database engine not available for monitoring")
            return
        
        @event.listens_for(engine, "connect")
        def receive_connect(dbapi_connection, connection_record):
            logger.debug("üîå Database connection established")
        
        @event.listens_for(engine, "checkout")
        def receive_checkout(dbapi_connection, connection_record, connection_proxy):
            logger.debug("üì§ Database connection checked out")
        
        @event.listens_for(engine, "checkin")
        def receive_checkin(dbapi_connection, connection_record):
            logger.debug("üì• Database connection checked in")
        
        self._monitoring_enabled = True
        logger.info("üìä Database connection pool monitoring enabled")
    
    def get_pool_status(self) -> Dict[str, Any]:
        """Get current connection pool status"""
        if not engine or not hasattr(engine, 'pool'):
            return {'error': 'Pool not available'}
        
        pool = engine.pool
        
        return {
            'pool_size': pool.size(),
            'checked_in': pool.checkedin(),
            'checked_out': pool.checkedout(),
            'overflow': pool.overflow(),
            'total_connections': pool.size() + pool.overflow(),
            'monitoring_enabled': self._monitoring_enabled
        }

# ============================================================================
# MAIN DATABASE SESSION MANAGER
# ============================================================================

class DatabaseSessionManager:
    """
    Main manager that coordinates user session caching, smart database sync,
    and connection pool optimization for maximum performance.
    """
    
    def __init__(self):
        # Initialize components
        self.session_cache = UserSessionCache()
        self.smart_sync = SmartDatabaseSync(self.session_cache)
        self.connection_optimizer = DatabaseConnectionOptimizer()
        
        # Performance monitoring
        self._total_operations = 0
        self._cache_performance = defaultdict(float)
        
        logger.info("üöÄ DatabaseSessionManager initialized")
    
    async def start_services(self):
        """Start background services"""
        await self.session_cache.start_background_cleanup()
        self.connection_optimizer.enable_pool_monitoring()
        logger.info("üéØ Database optimization services started")
    
    async def stop_services(self):
        """Stop background services"""
        await self.session_cache.stop_background_cleanup()
        logger.info("‚èπÔ∏è Database optimization services stopped")
    
    async def get_or_create_user_session(self, 
                                       clerk_user_data: Dict[str, Any], 
                                       db: Session,
                                       force_refresh: bool = False) -> UserSessionData:
        """
        Main method to get or create user session with optimal caching.
        
        Args:
            clerk_user_data: User data from Clerk
            db: Database session
            force_refresh: Force cache refresh
            
        Returns:
            UserSessionData with current user information
        """
        start_time = time.time()
        self._total_operations += 1
        
        clerk_user_id = clerk_user_data.get('id')
        if not clerk_user_id:
            raise ValueError("No Clerk user ID provided")
        
        try:
            # Try cache first (unless forced refresh)
            if not force_refresh:
                cached_session = self.session_cache.get_user_session(clerk_user_id)
                if cached_session and not cached_session.needs_sync(clerk_user_data):
                    operation_time = (time.time() - start_time) * 1000
                    self._cache_performance['cache_hit'] += operation_time
                    logger.debug(f"‚ö° Cache hit for user session: {clerk_user_id} ({operation_time:.1f}ms)")
                    return cached_session
            
            # Perform smart database sync
            session_data = await self.smart_sync.sync_user_with_database(
                clerk_user_data, db, force_sync=force_refresh
            )
            
            operation_time = (time.time() - start_time) * 1000
            self._cache_performance['database_sync'] += operation_time
            logger.debug(f"üîÑ Database sync for user session: {clerk_user_id} ({operation_time:.1f}ms)")
            
            return session_data
            
        except Exception as e:
            operation_time = (time.time() - start_time) * 1000
            self._cache_performance['error'] += operation_time
            logger.error(f"‚ùå User session operation failed: {e} ({operation_time:.1f}ms)")
            raise
    
    def invalidate_user_session(self, clerk_user_id: str) -> bool:
        """Invalidate cached user session"""
        return self.session_cache.invalidate_user_session(clerk_user_id)
    
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """Get comprehensive performance statistics"""
        cache_stats = self.session_cache.get_stats()
        sync_stats = self.smart_sync.get_sync_stats()
        pool_stats = self.connection_optimizer.get_pool_status()
        
        # Calculate average operation times
        avg_times = {}
        if self._total_operations > 0:
            for operation, total_time in self._cache_performance.items():
                avg_times[f'avg_{operation}_ms'] = total_time / self._total_operations
        
        return {
            'timestamp': datetime.now().isoformat(),
            'total_operations': self._total_operations,
            'cache_statistics': cache_stats,
            'sync_statistics': sync_stats,
            'connection_pool': pool_stats,
            'performance_averages': avg_times,
            'cache_efficiency': {
                'database_load_reduction': cache_stats.get('hit_rate', 0) * 100,
                'sync_skip_rate': sync_stats.get('skip_rate', 0) * 100
            }
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """Comprehensive health check"""
        health = {
            'status': 'healthy',
            'timestamp': datetime.now().isoformat(),
            'components': {}
        }
        
        try:
            # Check cache health
            cache_stats = self.session_cache.get_stats()
            health['components']['session_cache'] = {
                'status': 'healthy',
                'cache_size': cache_stats['cache_size'],
                'hit_rate': cache_stats['hit_rate']
            }
        except Exception as e:
            health['components']['session_cache'] = {
                'status': 'unhealthy',
                'error': str(e)
            }
            health['status'] = 'degraded'
        
        try:
            # Check database connection pool
            pool_stats = self.connection_optimizer.get_pool_status()
            health['components']['connection_pool'] = {
                'status': 'healthy',
                **pool_stats
            }
        except Exception as e:
            health['components']['connection_pool'] = {
                'status': 'unhealthy',
                'error': str(e)
            }
            health['status'] = 'degraded'
        
        return health

# ============================================================================
# GLOBAL INSTANCES
# ============================================================================

# Global database session manager
database_session_manager = DatabaseSessionManager()

# Convenience functions for easy integration
async def get_user_session_cached(clerk_user_data: Dict[str, Any], db: Session) -> UserSessionData:
    """Convenience function to get cached user session"""
    return await database_session_manager.get_or_create_user_session(clerk_user_data, db)

async def invalidate_user_cache(clerk_user_id: str) -> bool:
    """Convenience function to invalidate user cache"""
    return database_session_manager.invalidate_user_session(clerk_user_id)

async def get_database_performance_stats() -> Dict[str, Any]:
    """Convenience function to get performance statistics"""
    return database_session_manager.get_comprehensive_stats()

# ============================================================================
# STARTUP AND SHUTDOWN HOOKS
# ============================================================================

async def startup_database_optimization():
    """Initialize database optimization services"""
    await database_session_manager.start_services()
    logger.info("üöÄ Database optimization startup complete")

async def shutdown_database_optimization():
    """Cleanup database optimization services"""
    await database_session_manager.stop_services()
    logger.info("‚èπÔ∏è Database optimization shutdown complete")

# Export main interfaces
__all__ = [
    'UserSessionData',
    'UserSessionCache', 
    'SmartDatabaseSync',
    'DatabaseConnectionOptimizer',
    'DatabaseSessionManager',
    'database_session_manager',
    'get_user_session_cached',
    'invalidate_user_cache',
    'get_database_performance_stats',
    'startup_database_optimization',
    'shutdown_database_optimization'
]